{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72afb1-a7ca-4e7b-aa58-89a20dc83ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Gradient Boosting Regression?\n",
    "'''\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous numerical output\n",
    "based on input features. It's an ensemble learning method that combines the predictions of multiple weak learners (usually decision trees) to create a \n",
    "stronger predictive model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c391798b-1363-4fc4-8585-b23fc75cfe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7208\n",
      "R-squared: -0.2591\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate some example data\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the number of iterations and the learning rate\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the prediction with the mean of the target values\n",
    "y_pred = np.full(y_train.shape, np.mean(y_train))\n",
    "\n",
    "# Iterate to build the gradient boosting model\n",
    "for i in range(n_estimators):\n",
    "    # Calculate the residuals\n",
    "    residuals = y_train - y_pred\n",
    "    \n",
    "    # Fit a decision tree regressor to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(X_train, residuals)\n",
    "    \n",
    "    # Make predictions with the current tree\n",
    "    tree_pred = tree.predict(X_train)\n",
    "    \n",
    "    # Update the prediction with the scaled predictions from the current tree\n",
    "    y_pred += learning_rate * tree_pred\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = np.full(y_test.shape, np.mean(y_train))\n",
    "for i in range(n_estimators):\n",
    "    y_pred_test += learning_rate * tree.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d267e937-03bd-4b9e-874d-0a926e3bcccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150}\n",
      "Mean Squared Error: 0.3455\n",
      "R-squared: 0.3964\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_regressor = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best estimator\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207dffa-4683-4aff-b49e-c92a200a86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.What is a weak learner in Gradient Boosting?\n",
    "'''\n",
    "In Gradient Boosting, the weak learners are typically decision trees with limited depth. Each new weak learner focuses on the residuals (errors) of\n",
    "the combined model's predictions, helping to reduce the errors that the previous learners couldn't handle effectively. By combining these weak learners\n",
    "with an appropriate weighting, the overall model becomes much more accurate and capable of capturing complex relationships in the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ab052-5765-4459-8a68-2f374ede28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.What is the intuition behind the Gradient Boosting algorithm?\n",
    "'''\n",
    "Start with a Simple Model: The algorithm begins with a simple model, often a decision tree with just a few levels (shallow depth). This simple model is\n",
    "called the \"weak learner.\"\n",
    "\n",
    "Sequential Learning: Gradient Boosting works sequentially. In each iteration, it tries to improve upon the errors made by the model built in the \n",
    "previous iterations. The idea is to build a strong model by incrementally adding weak learners that are tailored to the data's errors.\n",
    "\n",
    "Focus on Residuals: At the beginning, the errors (residuals) made by the current model are significant. A new weak learner is then trained to predict\n",
    "these residuals, which captures the patterns in the data that the current model is struggling with.\n",
    "\n",
    "Correcting Errors: The predictions from the new weak learner are not added directly to the previous model's predictions. Instead, they are scaled by a \n",
    "small value (learning rate) and added to the previous predictions. This correction step helps in reducing the errors made by the previous model.\n",
    "\n",
    "Iterative Improvement: The algorithm continues to iterate, with each iteration adding a new weak learner to correct the model's errors. Over time, \n",
    "the collective predictions of these weak learners converge to a strong predictive model.\n",
    "\n",
    "Ensemble of Weak Learners: The final model is an ensemble of these weak learners, each focused on a specific aspect of the data. Together, they work\n",
    "collaboratively to capture the complex relationships and patterns within the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c922-08e7-4a30-b97e-6abcbc3bca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "'''\n",
    "Initialization: The process starts by initializing the ensemble with a simple model, often a weak learner like a shallow decision tree. This initial \n",
    "model provides the initial predictions.\n",
    "\n",
    "Calculate Residuals: The residuals are the differences between the actual target values and the predictions of the current ensemble. In the beginning,\n",
    "the residuals are substantial because the initial model's predictions are far from perfect.\n",
    "\n",
    "Fit a Weak Learner to Residuals: A new weak learner (another shallow decision tree) is trained to predict the residuals. This weak learner is \n",
    "constructed in a way that minimizes the residuals' error. It captures the patterns and relationships that the current ensemble hasn't learned well.\n",
    "\n",
    "Update Ensemble's Predictions: The predictions of the newly trained weak learner are scaled by a small factor called the learning rate and added to \n",
    "the predictions of the current ensemble. This step improves the ensemble's predictions by a fraction of the weak learner's predictions.\n",
    "\n",
    "Iterate: Steps 2 to 4 are repeated for a predefined number of iterations or until the performance of the ensemble converges to a desired level.\n",
    "\n",
    "Final Ensemble: The final ensemble model is the sum of the predictions made by all the weak learners, each scaled by the learning rate. This ensemble\n",
    "combines the predictions of all weak learners to create a single strong predictive model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4f219-d652-446c-8ff6-8ffb292420dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "'''\n",
    "Loss Function: Begin with a loss function that quantifies the error between the model's predictions and the actual target values. The choice of loss \n",
    "function depends on the problem, e.g., mean squared error for regression, log loss for classification.\n",
    "\n",
    "Initialize with a Constant: Start by initializing the ensemble with a constant value, often the mean of the target values. This initial prediction\n",
    "represents the baseline.\n",
    "\n",
    "Calculate Negative Gradient: Compute the negative gradient of the loss function with respect to the current ensemble's predictions. This gradient\n",
    "represents the direction and magnitude of change required to minimize the loss.\n",
    "\n",
    "Fit a Weak Learner to Negative Gradient: Train a new weak learner (usually a decision tree with shallow depth) to predict the negative gradient \n",
    "calculated in the previous step. This weak learner is designed to capture the patterns in the residuals (errors) that the current ensemble is \n",
    "struggling with.\n",
    "\n",
    "Update Ensemble's Predictions: Adjust the ensemble's predictions by adding the scaled predictions from the new weak learner. The scaling factor is \n",
    "the learning rate, which controls the step size of the updates.\n",
    "\n",
    "Repeat for Multiple Iterations: Iteratively repeat steps 3 to 5 for a specified number of iterations. In each iteration, compute the negative gradient,\n",
    "train a new weak learner, and update the ensemble's predictions.\n",
    "\n",
    "Final Ensemble Prediction: The final prediction of the Gradient Boosting model is the sum of the predictions from all the weak learners, scaled by \n",
    "the learning rate.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
