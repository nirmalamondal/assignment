{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fb284-3e84-4586-a5e8-2a2a9dd0ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "'''\n",
    "KNN is a simple and versatile supervised learning algorithm used for both classification and regression tasks. It operates based on similarity \n",
    "measures, where a new instance is classified or predicted by comparing it with the majority class or average value of its K nearest neighbors in the \n",
    "feature space.\n",
    "'''\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "'''\n",
    "The selection of the K value in KNN is crucial. Too small K values might lead to noise sensitivity, while too large K values might lead to \n",
    "over-smoothing. Common approaches for selecting K include cross-validation, trying different values and evaluating performance metrics (e.g., accuracy\n",
    "for classification, RMSE for regression), and considering the dataset's characteristics.\n",
    "'''\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "'''\n",
    "KNN Classifier: It predicts the class label of a new instance based on the majority class of its K nearest neighbors.\n",
    "KNN Regressor: It predicts the value of a new instance by averaging the values of its K nearest neighbors.\n",
    "'''\n",
    "Q4. How do you measure the performance of KNN?\n",
    "'''\n",
    "The performance of KNN is typically measured using various metrics:\n",
    "\n",
    "For classification: Accuracy, precision, recall, F1-score, ROC-AUC, etc.\n",
    "For regression: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, etc.\n",
    "'''\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "'''\n",
    "The curse of dimensionality refers to the deterioration of algorithm performance in high-dimensional spaces. In KNN, as the number of dimensions\n",
    "(features) increases, the feature space becomes increasingly sparse. This leads to a decrease in the efficacy of distance-based measures, as all data\n",
    "points appear to be similarly distant from each other in high dimensions\n",
    "'''\n",
    "Q6. How do you handle missing values in KNN?\n",
    "'''\n",
    "KNN can handle missing values by imputing them with the mean, median, or mode of the respective feature from the available neighboring data points\n",
    "(using distance-weighted methods). Another approach involves using algorithms that fill missing values before applying KNN.\n",
    "'''\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "'''\n",
    "KNN Classifier: Suitable for classification problems where decision boundaries are not linear and the number of classes is small to moderate.\n",
    "KNN Regressor: Suitable for regression problems where relationships between features and the target variable are non-linear and continuity matters.\n",
    "'''\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "'''\n",
    "Strengths:\n",
    "\n",
    "Simple and easy to understand.\n",
    "Effective in capturing non-linear relationships.\n",
    "No training phase, as it memorizes the entire training data.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive for large datasets.\n",
    "Sensitive to irrelevant and noisy features.\n",
    "Performance can be affected by the choice of K and distance metric.\n",
    "Addressing weaknesses involves feature selection/engineering, dimensionality reduction, appropriate scaling, and careful tuning of K and distance \n",
    "metrics.\n",
    "'''\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "'''\n",
    "Euclidean distance: It measures the shortest straight-line distance between two points in the feature space.\n",
    "Manhattan distance: It measures the distance as the sum of the absolute differences between the coordinates of the two points along each dimension. \n",
    "It's also known as L1 distance.\n",
    "'''\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "'''\n",
    "Feature scaling is crucial in KNN as it ensures that all features contribute equally to the distance calculation. Without scaling, features with \n",
    "larger scales dominate the distance calculation, leading to biased results. Common scaling techniques include normalization (scaling features to a \n",
    "similar range) or standardization (scaling features to have mean=0 and variance=1).\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
